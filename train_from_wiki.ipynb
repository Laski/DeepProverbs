{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train from wiki",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9Evc-RSH9S8",
        "outputId": "a7d4cf8b-db98-4951-8638-e8fcdbe7fae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%env FASTAI_HOME=/content\n",
        "# from https://medium.com/@pierre_guillou/faster-than-training-from-scratch-fine-tuning-the-english-gpt-2-in-any-language-with-hugging-f2ec05c98787"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: FASTAI_HOME=/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZN7i7vZrAjf",
        "outputId": "a40e04b5-75c6-4f2d-f087-4f5574e84781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Freeze versions of dependencies for now\n",
        "!pip install fastai2 fastcore==1.0.0\n",
        "!pip install tokenizers\n",
        "!pip install transformers\n",
        "\n",
        "from fastai2.text.all import *\n",
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastai2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/50/2f37212be57b7ee3e9c947336f75a66724468b21a3ca68734eaa82e7ebf3/fastai2-0.0.30-py3-none-any.whl (179kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 81kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 2.8MB/s \n",
            "\u001b[?25hCollecting fastcore==1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/92/233661d730b5613b4daf473cd28005bf2294fb1a858ce0bac57fbb7fa5ec/fastcore-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai2) (20.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai2) (3.13)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai2) (1.4.1)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from fastai2) (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai2) (3.2.2)\n",
            "Requirement already satisfied: torchvision>=0.7 in /usr/local/lib/python3.6/dist-packages (from fastai2) (0.7.0+cu101)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from fastai2) (19.3.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai2) (1.0.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from fastai2) (0.22.2.post1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from fastai2) (1.6.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai2) (2.23.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from fastai2) (7.0.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from fastai2) (2.2.4)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from fastcore==1.0.0) (0.7)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (from fastcore==1.0.0) (0.35.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastcore==1.0.0) (1.18.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastai2) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->fastai2) (1.15.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai2) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai2) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai2) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai2) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fastai2) (0.16.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->fastai2) (0.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai2) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai2) (3.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (50.3.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (0.8.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->fastai2) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->fastai2) (3.1.0)\n",
            "Installing collected packages: fastcore, fastai2\n",
            "Successfully installed fastai2-0.0.30 fastcore-1.0.0\n",
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ee/fedc3509145ad60fe5b418783f4a4c1b5462a4f0e8c7bbdbda52bdcda486/tokenizers-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.8.1\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/fc/18e56e5b1093052bacf6750442410423f3d9785d14ce4f54ab2ac6b112a6/transformers-3.3.0-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 2.8MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 10.8MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 31.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=a88bdbefb384c8362a3de23f33e8a9fd2d0ff43eda21fdd32ac3acf3356b05a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "  Found existing installation: tokenizers 0.8.1\n",
            "    Uninstalling tokenizers-0.8.1:\n",
            "      Successfully uninstalled tokenizers-0.8.1\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5Ur8SXEISMy",
        "outputId": "56da324f-2125-4848-b3a9-44178ad6db28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "lang = 'es'\n",
        "name = f'{lang}wiki'\n",
        "config = Config()\n",
        "data_path = config['data_path']\n",
        "path_data = data_path/name\n",
        "path_data.mkdir(exist_ok=True, parents=True)\n",
        "path_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('/content/data/eswiki')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cChClJ9vrxCu"
      },
      "source": [
        "# source: https://github.com/fastai/course-nlp/blob/master/nlputils.py\n",
        "\n",
        "from fastai2.basics import *\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def get_wiki(path,lang):\n",
        "    name = f'{lang}wiki'\n",
        "    if (path/name).exists():\n",
        "        print(f\"{path/name} already exists; not downloading\")\n",
        "        return\n",
        "\n",
        "    xml_fn = f\"{lang}wiki-latest-pages-articles.xml\"\n",
        "    zip_fn = f\"{xml_fn}.bz2\"\n",
        "\n",
        "    if not (path/xml_fn).exists():\n",
        "        print(\"downloading...\")\n",
        "        download_url(f'https://dumps.wikimedia.org/{name}/latest/{zip_fn}', path/zip_fn)\n",
        "        print(\"unzipping...\")\n",
        "        bunzip(path/zip_fn)\n",
        "\n",
        "    # Change working directory to `path`\n",
        "    prev_cwd = Path.cwd()\n",
        "    os.chdir(path)\n",
        "    \n",
        "    # Get wikiextractor\n",
        "    #if not (path/'wikiextractor').exists(): os.system('git clone https://github.com/attardi/wikiextractor.git')\n",
        "    # Extraction\n",
        "    print(\"extracting...\")\n",
        "    try:\n",
        "        from wikiextractor import WikiExtractor\n",
        "    except ImportError:\n",
        "        os.system('pip install wikiextractor')\n",
        "\n",
        "    os.system(\"python -m wikiextractor.WikiExtractor --processes 4 --no_templates \" + f\"--min_text_length 1800 --filter_disambig_pages --log_file log -b 100G -q {xml_fn}\")\n",
        "    shutil.move(str(path/'text/AA/wiki_00'), str(path/name))\n",
        "    shutil.rmtree(path/'text')\n",
        "    \n",
        "    # Return working directory to previous\n",
        "    os.chdir(prev_cwd)\n",
        "\n",
        "def split_wiki(path,lang):\n",
        "    dest = path/'docs'\n",
        "    name = f'{lang}wiki'\n",
        "    if dest.exists():\n",
        "        print(f\"{dest} already exists; not splitting\")\n",
        "        return dest\n",
        "\n",
        "    dest.mkdir(exist_ok=True, parents=True)\n",
        "    title_re = re.compile(rf'<doc id=\"\\d+\" url=\"https://{lang}.wikipedia.org/wiki\\?curid=\\d+\" title=\"([^\"]+)\">')\n",
        "    lines = (path/name).open()\n",
        "    f=None\n",
        "\n",
        "    for i,l in enumerate(lines):\n",
        "        if i%100000 == 0: print(i)\n",
        "        if l.startswith('<doc id=\"'):\n",
        "            title = title_re.findall(l)[0].replace('/','_')\n",
        "            if len(title)>150: continue\n",
        "            if f: f.close()\n",
        "            f = (dest/f'{title}.txt').open('w')\n",
        "        else: f.write(l)\n",
        "    f.close()\n",
        "    return dest\n",
        "\n",
        "def clean_files(dest):\n",
        "\n",
        "    doc_re = re.compile(rf'([\\w\\W]*)<\\/doc>') # delete </doc>\n",
        "    \n",
        "    for i,l in enumerate(dest.ls()):\n",
        "        # open file and get content without first line which is the title\n",
        "        f = l.open('r+', encoding=\"utf-8\")\n",
        "        f.readline()\n",
        "        text = f.read()\n",
        "        # get content without </doc> and delete empty line and whitespaces at the head and tail\n",
        "        text = doc_re.findall(text)[0].strip()\n",
        "        # delete file content\n",
        "        f.seek(0)\n",
        "        f.truncate()\n",
        "        # write modificated text in file\n",
        "        f.write(text)\n",
        "        f.close()\n",
        "        \n",
        "def get_one_clean_file(dest,lang):\n",
        "\n",
        "    fname = f'all_texts_{lang}wiki.txt'\n",
        "    doc_re = re.compile(rf'([\\w\\W]*)<\\/doc>') # delete </doc>\n",
        "    \n",
        "    all_texts = ''\n",
        "    for i,l in enumerate(dest.ls()):\n",
        "        # open file and get content without first line which is the title\n",
        "        f = l.open('r+', encoding=\"utf-8\")\n",
        "        f.readline()\n",
        "        text = f.read()\n",
        "        f.close()\n",
        "        # get content without </doc> and delete empty line and whitespaces at the head and tail\n",
        "        text = doc_re.findall(text)[0].strip()\n",
        "        # concatenate text\n",
        "        all_texts += text\n",
        "        all_texts += \"\\n\"\n",
        "        if not (i % 1000): print(i)\n",
        "  \n",
        "    with open (dest.parent/fname, 'w') as fp: \n",
        "        fp.write(all_texts)\n",
        "    print(f\"all texts from wikipedia {lang} in the file {dest.parent/fname}\\n\")\n",
        "\n",
        "def get_one_clean_csv_file(dest,lang):    \n",
        "                         \n",
        "    fname = f'all_texts_{lang}wiki.csv'\n",
        "    doc_re = re.compile(rf'([\\w\\W]*)<\\/doc>') # delete </doc>\n",
        "    \n",
        "    all_texts = list()\n",
        "    for i,l in enumerate(dest.ls()):\n",
        "        # open file and get content without first line which is the title\n",
        "        f = l.open('r+', encoding=\"utf-8\")\n",
        "        f.readline()\n",
        "        text = f.read()\n",
        "        f.close()\n",
        "        # get content without </doc> and delete empty line and whitespaces at the head and tail\n",
        "        text = doc_re.findall(text)[0].strip()\n",
        "        # append text\n",
        "        all_texts.append(text)\n",
        "  \n",
        "    # Create the pandas DataFrame \n",
        "    df = pd.DataFrame(all_texts, columns = ['text'])\n",
        "    \n",
        "    # save\n",
        "    df.to_csv(dest.parent/fname, index=False)  \n",
        "    print(f\"all texts from wikipedia {lang} in the file {dest.parent/fname}\\n\")\n",
        "                         \n",
        "def get_num_tokens(dest):\n",
        "    \n",
        "    # Getting an idea of the number of words\n",
        "    files = dest.ls()\n",
        "    num_tokens = 0\n",
        "\n",
        "    for i,l in enumerate(files):\n",
        "        f = l.open('r', encoding=\"utf-8\")\n",
        "        words = f.read()\n",
        "        num_tokens += len(words.split())\n",
        "        f.close()\n",
        "        \n",
        "    num_files = i+1\n",
        "    \n",
        "    return num_files, num_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwTncx4K_74z",
        "outputId": "5862c045-c7fa-4b42-f520-f9e432d79d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "get_wiki(path_data, lang)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "unzipping...\n",
            "extracting...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtrG06NtA9Fa",
        "outputId": "0a39e393-858c-4993-9865-caf71083e4ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# create one text file by article\n",
        "dest = split_wiki(path_data,lang)\n",
        "# get all articles in one text file and one csv file\n",
        "get_one_clean_file(dest,lang)\n",
        "get_one_clean_csv_file(dest,lang)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100000\n",
            "200000\n",
            "300000\n",
            "400000\n",
            "500000\n",
            "600000\n",
            "700000\n",
            "800000\n",
            "900000\n",
            "1000000\n",
            "1100000\n",
            "1200000\n",
            "1300000\n",
            "1400000\n",
            "1500000\n",
            "1600000\n",
            "1700000\n",
            "1800000\n",
            "1900000\n",
            "2000000\n",
            "2100000\n",
            "2200000\n",
            "2300000\n",
            "2400000\n",
            "2500000\n",
            "2600000\n",
            "2700000\n",
            "2800000\n",
            "2900000\n",
            "3000000\n",
            "3100000\n",
            "3200000\n",
            "3300000\n",
            "3400000\n",
            "3500000\n",
            "3600000\n",
            "3700000\n",
            "3800000\n",
            "3900000\n",
            "4000000\n",
            "4100000\n",
            "4200000\n",
            "4300000\n",
            "4400000\n",
            "4500000\n",
            "4600000\n",
            "4700000\n",
            "4800000\n",
            "4900000\n",
            "5000000\n",
            "5100000\n",
            "5200000\n",
            "5300000\n",
            "5400000\n",
            "5500000\n",
            "5600000\n",
            "5700000\n",
            "5800000\n",
            "5900000\n",
            "6000000\n",
            "6100000\n",
            "6200000\n",
            "6300000\n",
            "6400000\n",
            "6500000\n",
            "6600000\n",
            "6700000\n",
            "6800000\n",
            "6900000\n",
            "7000000\n",
            "7100000\n",
            "7200000\n",
            "7300000\n",
            "7400000\n",
            "7500000\n",
            "7600000\n",
            "7700000\n",
            "7800000\n",
            "7900000\n",
            "8000000\n",
            "8100000\n",
            "8200000\n",
            "8300000\n",
            "8400000\n",
            "8500000\n",
            "8600000\n",
            "8700000\n",
            "8800000\n",
            "8900000\n",
            "9000000\n",
            "9100000\n",
            "9200000\n",
            "9300000\n",
            "9400000\n",
            "9500000\n",
            "9600000\n",
            "9700000\n",
            "9800000\n",
            "9900000\n",
            "10000000\n",
            "10100000\n",
            "10200000\n",
            "10300000\n",
            "10400000\n",
            "10500000\n",
            "10600000\n",
            "10700000\n",
            "10800000\n",
            "10900000\n",
            "11000000\n",
            "11100000\n",
            "11200000\n",
            "11300000\n",
            "11400000\n",
            "11500000\n",
            "11600000\n",
            "11700000\n",
            "11800000\n",
            "11900000\n",
            "12000000\n",
            "12100000\n",
            "12200000\n",
            "12300000\n",
            "12400000\n",
            "12500000\n",
            "12600000\n",
            "12700000\n",
            "12800000\n",
            "12900000\n",
            "13000000\n",
            "13100000\n",
            "13200000\n",
            "13300000\n",
            "13400000\n",
            "13500000\n",
            "13600000\n",
            "13700000\n",
            "13800000\n",
            "13900000\n",
            "14000000\n",
            "14100000\n",
            "14200000\n",
            "14300000\n",
            "14400000\n",
            "14500000\n",
            "14600000\n",
            "14700000\n",
            "14800000\n",
            "14900000\n",
            "15000000\n",
            "15100000\n",
            "15200000\n",
            "15300000\n",
            "15400000\n",
            "15500000\n",
            "15600000\n",
            "15700000\n",
            "15800000\n",
            "15900000\n",
            "16000000\n",
            "16100000\n",
            "16200000\n",
            "16300000\n",
            "16400000\n",
            "16500000\n",
            "16600000\n",
            "16700000\n",
            "16800000\n",
            "16900000\n",
            "17000000\n",
            "17100000\n",
            "17200000\n",
            "17300000\n",
            "17400000\n",
            "17500000\n",
            "17600000\n",
            "17700000\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n",
            "68000\n",
            "69000\n",
            "70000\n",
            "71000\n",
            "72000\n",
            "73000\n",
            "74000\n",
            "75000\n",
            "76000\n",
            "77000\n",
            "78000\n",
            "79000\n",
            "80000\n",
            "81000\n",
            "82000\n",
            "83000\n",
            "84000\n",
            "85000\n",
            "86000\n",
            "87000\n",
            "88000\n",
            "89000\n",
            "90000\n",
            "91000\n",
            "92000\n",
            "93000\n",
            "94000\n",
            "95000\n",
            "96000\n",
            "97000\n",
            "98000\n",
            "99000\n",
            "100000\n",
            "101000\n",
            "102000\n",
            "103000\n",
            "104000\n",
            "105000\n",
            "106000\n",
            "107000\n",
            "108000\n",
            "109000\n",
            "110000\n",
            "111000\n",
            "112000\n",
            "113000\n",
            "114000\n",
            "115000\n",
            "116000\n",
            "117000\n",
            "118000\n",
            "119000\n",
            "120000\n",
            "121000\n",
            "122000\n",
            "123000\n",
            "124000\n",
            "125000\n",
            "126000\n",
            "127000\n",
            "128000\n",
            "129000\n",
            "130000\n",
            "131000\n",
            "132000\n",
            "133000\n",
            "134000\n",
            "135000\n",
            "136000\n",
            "137000\n",
            "138000\n",
            "139000\n",
            "140000\n",
            "141000\n",
            "142000\n",
            "143000\n",
            "144000\n",
            "145000\n",
            "146000\n",
            "147000\n",
            "148000\n",
            "149000\n",
            "150000\n",
            "151000\n",
            "152000\n",
            "153000\n",
            "154000\n",
            "155000\n",
            "156000\n",
            "157000\n",
            "158000\n",
            "159000\n",
            "160000\n",
            "161000\n",
            "162000\n",
            "163000\n",
            "164000\n",
            "165000\n",
            "166000\n",
            "167000\n",
            "168000\n",
            "169000\n",
            "170000\n",
            "171000\n",
            "172000\n",
            "173000\n",
            "174000\n",
            "175000\n",
            "176000\n",
            "177000\n",
            "178000\n",
            "179000\n",
            "180000\n",
            "181000\n",
            "182000\n",
            "183000\n",
            "184000\n",
            "185000\n",
            "186000\n",
            "187000\n",
            "188000\n",
            "189000\n",
            "190000\n",
            "191000\n",
            "192000\n",
            "193000\n",
            "194000\n",
            "195000\n",
            "196000\n",
            "197000\n",
            "198000\n",
            "199000\n",
            "200000\n",
            "201000\n",
            "202000\n",
            "203000\n",
            "204000\n",
            "205000\n",
            "206000\n",
            "207000\n",
            "208000\n",
            "209000\n",
            "210000\n",
            "211000\n",
            "212000\n",
            "213000\n",
            "214000\n",
            "215000\n",
            "216000\n",
            "217000\n",
            "218000\n",
            "219000\n",
            "220000\n",
            "221000\n",
            "222000\n",
            "223000\n",
            "224000\n",
            "225000\n",
            "226000\n",
            "227000\n",
            "228000\n",
            "229000\n",
            "230000\n",
            "231000\n",
            "232000\n",
            "233000\n",
            "234000\n",
            "235000\n",
            "236000\n",
            "237000\n",
            "238000\n",
            "239000\n",
            "240000\n",
            "241000\n",
            "242000\n",
            "243000\n",
            "244000\n",
            "245000\n",
            "246000\n",
            "247000\n",
            "248000\n",
            "249000\n",
            "250000\n",
            "251000\n",
            "252000\n",
            "253000\n",
            "254000\n",
            "255000\n",
            "256000\n",
            "257000\n",
            "258000\n",
            "259000\n",
            "260000\n",
            "261000\n",
            "262000\n",
            "263000\n",
            "264000\n",
            "265000\n",
            "266000\n",
            "267000\n",
            "268000\n",
            "269000\n",
            "270000\n",
            "271000\n",
            "272000\n",
            "273000\n",
            "274000\n",
            "275000\n",
            "276000\n",
            "277000\n",
            "278000\n",
            "279000\n",
            "280000\n",
            "281000\n",
            "282000\n",
            "283000\n",
            "284000\n",
            "285000\n",
            "286000\n",
            "287000\n",
            "288000\n",
            "289000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHPlogciBJ_K"
      },
      "source": [
        "%%time\n",
        "# Size of downloaded data in the docs folder\n",
        "num_files, num_tokens = get_num_tokens(dest)\n",
        "print(f'{num_files} files - {num_tokens} tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2NkZEh3EM9k"
      },
      "source": [
        "# 1. Get the pre-trained GPT2 Tokenizer (pre-trained with an English\n",
        "# corpus) from the Transformers library (Hugging Face) \n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "pretrained_weights = 'gpt2'\n",
        "tokenizer_en = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
        "tokenizer_en.pad_token = tokenizer_en.eos_token\n",
        "\n",
        "# 2. Train a Byte Level BPE (BBPE) tokenizer on the Spanish\n",
        "# Wikipedia corpus by using the Tokenizers library (Hugging Face)\n",
        "\n",
        "# 2.1 Get GPT2 tokenizer_en vocab size\n",
        "ByteLevelBPE_tokenizer_es_vocab_size = tokenizer_en.vocab_size\n",
        "ByteLevelBPE_tokenizer_es_vocab_size\n",
        "\n",
        "# 2.2 ByteLevelBPETokenizer Represents a Byte-level BPE\n",
        "# as introduced by OpenAI with their GPT-2 model\n",
        "ByteLevelBPE_tokenizer_es = ByteLevelBPETokenizer()\n",
        "\n",
        "# 2.3 Get list of paths to corpus files\n",
        "# and customize training with <|endoftext|> special GPT-2 token\n",
        "paths = [str(path_data/'all_texts_eswiki.txt')]\n",
        "ByteLevelBPE_tokenizer_es.train(files=paths,\n",
        "                    vocab_size=ByteLevelBPE_tokenizer_es_vocab_size, \n",
        "                    min_frequency=2, \n",
        "                    special_tokens=[\"<|endoftext|>\"])\n",
        "# Get sequence length max of 1024\n",
        "ByteLevelBPE_tokenizer_es.enable_truncation(max_length=1024)\n",
        "\n",
        "# 2.4 save tokenizer\n",
        "ByteLevelBPE_tokenizer_es_rep = 'ByteLevelBPE_tokenizer_es'\n",
        "path_to_ByteLevelBPE_tokenizer_es_rep = path_data/ByteLevelBPE_tokenizer_es_rep\n",
        "if not (path_to_ByteLevelBPE_tokenizer_es_rep).exists():\n",
        "    path_to_ByteLevelBPE_tokenizer_es_rep.mkdir(exist_ok=True, parents=True)\n",
        "ByteLevelBPE_tokenizer_es.save_model(str(path_to_ByteLevelBPE_tokenizer_es_rep))\n",
        "\n",
        "# 3. Import the tokenizer config files in Spanish into the pre-trained GPT2 Tokenizer\n",
        "tokenizer_pt = GPT2TokenizerFast.from_pretrained(\n",
        "    str(path_to_ByteLevelBPE_tokenizer_es_rep), \n",
        "    pad_token='<|endoftext|>')\n",
        "# Get sequence length max of 1024\n",
        "tokenizer_es.model_max_length = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9_NCZwBNub1"
      },
      "source": [
        "# 1. GPT2TokenizerFast (imported GPT-2 tokenizer) → fastai Tokenizer\n",
        "class TransformersTokenizer(Transform):\n",
        "    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
        "    def encodes(self, x): \n",
        "        toks = self.tokenizer.tokenize(x)\n",
        "        return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n",
        "    def decodes(self, x):\n",
        "      return TitledStr(self.tokenizer.decode(x.cpu().numpy()))\n",
        "    \n",
        "tokenizer_fastai_en = TransformersTokenizer(tokenizer_en)\n",
        "tokenizer_fastai_es = TransformersTokenizer(tokenizer_es)\n",
        "\n",
        "\n",
        "# 2. Change vocab embedding in the GPT-2 pre-trained model to adapt to the Portuguese vocab\n",
        "# Get weights of the old wte\n",
        "old_wgts = model.transformer.get_input_embeddings().weight.clone().detach()\n",
        "\n",
        "# Get the mean embedding vector of the old wte\n",
        "wgts_m = old_wgts.mean(0)\n",
        "\n",
        "# Initialize vocab size and weights of the new wte\n",
        "new_vocab_size = tokenizer_fastai_es.tokenizer.vocab_size\n",
        "new_wgts = old_wgts.new_zeros(new_vocab_size,old_wgts.size(1))\n",
        "\n",
        "# Get the new wte keeping the embedding vectors of tokens \n",
        "# in common in the 2 vocabs\n",
        "# A token present in the new vocab but not in the old one \n",
        "# gets the mean embedding vector of the old wte\n",
        "old_vocab = tokenizer_fastai_en.tokenizer.get_vocab()\n",
        "new_vocab = tokenizer_fastai_es.tokenizer.get_vocab()\n",
        "same_tokens_list = list()\n",
        "different_tokens_list = list()\n",
        "    \n",
        "for w,idx_new in new_vocab.items():    \n",
        "    idx_old = old_vocab.get(w, -1)\n",
        "    if idx_old>=0:\n",
        "        new_wgts[idx_new] = old_wgts[idx_old]\n",
        "        same_tokens_list.append((w,idx_new))\n",
        "    else:\n",
        "        new_wgts[idx_new] = wgts_m\n",
        "        different_tokens_list.append((w,idx_new))\n",
        "        \n",
        "# setup in model the new wte\n",
        "new_wte = nn.Embedding(new_vocab_size,old_wgts.size(1))\n",
        "new_wte.weight.data = new_wgts\n",
        "model.transformer.set_input_embeddings(new_wte)\n",
        "\n",
        "# save new_wgts\n",
        "torch.save(new_wgts, path_data/'new_wte_wgts.es')\n",
        "# save same_tokens_list and different_tokens_list\n",
        "torch.save(same_tokens_list, path_data/'same_tokens_list.es')\n",
        "torch.save(different_tokens_list, path_data/'different_tokens_list.es')\n",
        "\n",
        "# Changing lm_head weights with the new embedding\n",
        "matrixmodel.lm_head.weight = model.transformer.wte.weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imqNRnnuTVEq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}