{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "r9Evc-RSH9S8",
    "outputId": "a7d4cf8b-db98-4951-8638-e8fcdbe7fae6"
   },
   "outputs": [],
   "source": [
    "%env FASTAI_HOME=.\n",
    "# from https://medium.com/@pierre_guillou/faster-than-training-from-scratch-fine-tuning-the-english-gpt-2-in-any-language-with-hugging-f2ec05c98787"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DZN7i7vZrAjf",
    "outputId": "a40e04b5-75c6-4f2d-f087-4f5574e84781",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Freeze versions of dependencies for now\n",
    "!pip install fastai2==0.0.30 fastcore==1.0.0 tokenizers==0.8.1rc2 transformers==3.3.1 torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "\n",
    "from fastai2.text.all import *\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO\n",
    "    )\n",
    "\n",
    "DOWNLOAD_WIKI = False\n",
    "GET_SHARED_TOKENS = False\n",
    "TOKENIZE = False\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "c5Ur8SXEISMy",
    "outputId": "56da324f-2125-4848-b3a9-44178ad6db28"
   },
   "outputs": [],
   "source": [
    "lang = 'es'\n",
    "name = f'{lang}wiki'\n",
    "config = Config()\n",
    "data_path = config['data_path']\n",
    "path_data = data_path/name\n",
    "path_data.mkdir(exist_ok=True, parents=True)\n",
    "path_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki download and extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cChClJ9vrxCu"
   },
   "outputs": [],
   "source": [
    "# source: https://github.com/fastai/course-nlp/blob/master/nlputils.py\n",
    "\n",
    "from fastai2.basics import *\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_wiki(path,lang):\n",
    "    name = f'{lang}wiki'\n",
    "    if (path/name).exists():\n",
    "        print(f\"{path/name} already exists; not downloading\")\n",
    "        return\n",
    "\n",
    "    xml_fn = f\"{lang}wiki-latest-pages-articles.xml\"\n",
    "    zip_fn = f\"{xml_fn}.bz2\"\n",
    "\n",
    "    if not (path/xml_fn).exists():\n",
    "        print(\"downloading...\")\n",
    "        download_url(f'https://dumps.wikimedia.org/{name}/latest/{zip_fn}', path/zip_fn)\n",
    "        print(\"unzipping...\")\n",
    "        bunzip(path/zip_fn)\n",
    "\n",
    "    # Get wikiextractor\n",
    "    #if not (path/'wikiextractor').exists(): os.system('git clone https://github.com/attardi/wikiextractor.git')\n",
    "    # Extraction\n",
    "\n",
    "    if not (path/'text/AA/wiki_00').exists():\n",
    "        print(\"extracting...\")\n",
    "        try:\n",
    "            from wikiextractor import WikiExtractor\n",
    "        except ImportError:\n",
    "            os.system('pip install wikiextractor')\n",
    "        os.system(f\"python -m wikiextractor.WikiExtractor --processes 4 --no_templates --min_text_length 1800 --filter_disambig_pages --log_file log -b 100G -q {path/xml_fn}\")\n",
    "    \n",
    "    dest = path/name\n",
    "    dest.mkdir(exist_ok=True, parents=True)\n",
    "    shutil.move(str(path/'text/AA/wiki_00'), str(dest))\n",
    "    shutil.rmtree(path/'text')\n",
    "    return dest\n",
    "\n",
    "def split_wiki(path,lang):\n",
    "    dest = path/'docs'\n",
    "    name = f'{lang}wiki'\n",
    "    if dest.exists():\n",
    "        print(f\"{dest} already exists; not splitting\")\n",
    "        return dest\n",
    "\n",
    "    dest.mkdir(exist_ok=True, parents=True)\n",
    "    title_re = re.compile(rf'<doc id=\"\\d+\" url=\"https://{lang}.wikipedia.org/wiki\\?curid=\\d+\" title=\"([^\"]+)\">')\n",
    "    lines = (path/name).open()\n",
    "    f=None\n",
    "    \n",
    "    for i,l in enumerate(lines):\n",
    "        if i%100000 == 0: print(i)\n",
    "        if l.startswith('<doc id=\"'):\n",
    "            title = title_re.findall(l)[0].replace('/','_')\n",
    "            if len(title)>150: continue\n",
    "            if f: f.close()\n",
    "            f = (dest/f'{title}.txt').open('w')\n",
    "        else: f.write(l)\n",
    "    f.close()\n",
    "    return dest\n",
    "\n",
    "def clean_files(dest):\n",
    "\n",
    "    doc_re = re.compile(rf'([\\w\\W]*)<\\/doc>') # delete </doc>\n",
    "    \n",
    "    for i,l in enumerate(dest.ls()):\n",
    "        # open file and get content without first line which is the title\n",
    "        f = l.open('r+', encoding=\"utf-8\")\n",
    "        f.readline()\n",
    "        text = f.read()\n",
    "        # get content without </doc> and delete empty line and whitespaces at the head and tail\n",
    "        text = doc_re.findall(text)[0].strip()\n",
    "        # delete file content\n",
    "        f.seek(0)\n",
    "        f.truncate()\n",
    "        # write modificated text in file\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        \n",
    "def get_one_clean_file(dest,lang):\n",
    "\n",
    "    fname = f'all_texts_{lang}wiki.txt'\n",
    "    doc_re = re.compile(rf'([\\w\\W]*)<\\/doc>') # delete </doc>\n",
    "    \n",
    "    all_texts = ''\n",
    "\n",
    "    with open (dest.parent/fname, 'w') as fp: \n",
    "        for i,l in enumerate(dest.ls()):\n",
    "            # open file and get content without first line which is the title\n",
    "            f = l.open('r+', encoding=\"utf-8\")\n",
    "            f.readline()\n",
    "            text = f.read()\n",
    "            f.close()\n",
    "            # get content without </doc> and delete empty line and whitespaces at the head and tail\n",
    "            text = doc_re.findall(text)[0].strip()\n",
    "            # concatenate text\n",
    "            fp.write(text)\n",
    "            fp.write(\"\\n\")\n",
    "            if not (i % 1000): print(i)\n",
    " \n",
    "    print(f\"all texts from wikipedia {lang} in the file {dest.parent/fname}\\n\")\n",
    "\n",
    "def get_one_clean_csv_file(dest,lang):    \n",
    "                         \n",
    "    fname = f'all_texts_{lang}wiki.csv'\n",
    "    doc_re = re.compile(rf'([\\w\\W]*)<\\/doc>') # delete </doc>\n",
    "    \n",
    "    all_texts = list()\n",
    "    for i,l in enumerate(dest.ls()):\n",
    "        # open file and get content without first line which is the title\n",
    "        f = l.open('r+', encoding=\"utf-8\")\n",
    "        f.readline()\n",
    "        text = f.read()\n",
    "        f.close()\n",
    "        # get content without </doc> and delete empty line and whitespaces at the head and tail\n",
    "        text = doc_re.findall(text)[0].strip()\n",
    "        # append text\n",
    "        all_texts.append(text)\n",
    "  \n",
    "    # Create the pandas DataFrame \n",
    "    df = pd.DataFrame(all_texts, columns = ['text'])\n",
    "    \n",
    "    # save\n",
    "    df.to_csv(dest.parent/fname, index=False)  \n",
    "    print(f\"all texts from wikipedia {lang} in the file {dest.parent/fname}\\n\")\n",
    "                         \n",
    "def get_num_tokens(dest):\n",
    "    \n",
    "    # Getting an idea of the number of words\n",
    "    files = dest.ls()\n",
    "    num_tokens = 0\n",
    "\n",
    "    for i,l in enumerate(files):\n",
    "        f = l.open('r', encoding=\"utf-8\")\n",
    "        words = f.read()\n",
    "        num_tokens += len(words.split())\n",
    "        f.close()\n",
    "        \n",
    "    num_files = i+1\n",
    "    \n",
    "    return num_files, num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "CwTncx4K_74z",
    "outputId": "5862c045-c7fa-4b42-f520-f9e432d79d08"
   },
   "outputs": [],
   "source": [
    "if DOWNLOAD_WIKI:\n",
    "    get_wiki(path_data, lang)\n",
    "    print(\"create one text file by article\")\n",
    "    dest = split_wiki(path_data,lang)\n",
    "    print(\"get all articles in one text file and one csv file\")\n",
    "    get_one_clean_file(dest,lang)\n",
    "    get_one_clean_csv_file(dest,lang)\n",
    "    # Size of downloaded data in the docs folder\n",
    "    num_files, num_tokens = get_num_tokens(dest)\n",
    "    print(f'{num_files} files - {num_tokens} tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "# The GPT2 Model transformer with a language modeling head on top\n",
    "# (linear layer with weights tied to the input embeddings)\n",
    "\n",
    "# GPT2Tokenizer: https://huggingface.co/transformers/model_doc/gpt2.html#gpt2tokenizer\n",
    "# GPT2TokenizerFast: https://huggingface.co/transformers/model_doc/gpt2.html#gpt2tokenizerfast\n",
    "# GPT2LMHeadModel: https://huggingface.co/transformers/model_doc/gpt2.html#transformers.GPT2LMHeadModel\n",
    "\n",
    "pretrained_weights = 'gpt2'\n",
    "tokenizer_en = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "model_en = GPT2LMHeadModel.from_pretrained(pretrained_weights)\n",
    "# To correct the warning about token_pad (GPT2TokenizerFast), run the following code\n",
    "# source: https://github.com/huggingface/transformers/issues/2648#issuecomment-616177044\n",
    "tokenizer_en.pad_token = tokenizer_en.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2NkZEh3EM9k"
   },
   "outputs": [],
   "source": [
    "if GET_SHARED_TOKENS:\n",
    "    # 1. Get the pre-trained GPT2 Tokenizer (pre-trained with an English\n",
    "    # corpus) from the Transformers library (Hugging Face) \n",
    "    from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "    pretrained_weights = 'gpt2'\n",
    "    tokenizer_en = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "    tokenizer_en.pad_token = tokenizer_en.eos_token\n",
    "\n",
    "    # 2. Train a Byte Level BPE (BBPE) tokenizer on the Spanish\n",
    "    # Wikipedia corpus by using the Tokenizers library (Hugging Face)\n",
    "\n",
    "    # 2.1 Get GPT2 tokenizer_en vocab size\n",
    "    ByteLevelBPE_tokenizer_es_vocab_size = tokenizer_en.vocab_size\n",
    "    ByteLevelBPE_tokenizer_es_vocab_size\n",
    "\n",
    "    # 2.2 ByteLevelBPETokenizer Represents a Byte-level BPE\n",
    "    # as introduced by OpenAI with their GPT-2 model\n",
    "    ByteLevelBPE_tokenizer_es = ByteLevelBPETokenizer()\n",
    "\n",
    "    # 2.3 Get list of paths to corpus files\n",
    "    # and customize training with <|endoftext|> special GPT-2 token\n",
    "    paths = [str(path_data/'all_texts_eswiki.txt')]\n",
    "    ByteLevelBPE_tokenizer_es.train(files=paths,\n",
    "                        vocab_size=ByteLevelBPE_tokenizer_es_vocab_size, \n",
    "                        min_frequency=2, \n",
    "                        special_tokens=[\"<|endoftext|>\"])\n",
    "\n",
    "    # Get sequence length max of 1024\n",
    "    ByteLevelBPE_tokenizer_es.enable_truncation(max_length=1024)\n",
    "\n",
    "    # 2.4 save tokenizer\n",
    "    path_to_ByteLevelBPE_tokenizer_es_rep = path_data/'ByteLevelBPE_tokenizer_es'\n",
    "    if not (path_to_ByteLevelBPE_tokenizer_es_rep).exists():\n",
    "        path_to_ByteLevelBPE_tokenizer_es_rep.mkdir(exist_ok=True, parents=True)\n",
    "    ByteLevelBPE_tokenizer_es.save_model(str(path_to_ByteLevelBPE_tokenizer_es_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer ByteLevelBPE_tokenizer_es\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# Get the path to ByteLevelBPE_tokenizer_ES config files\n",
    "path_to_ByteLevelBPE_tokenizer_es_rep = path_data/'ByteLevelBPE_tokenizer_es'\n",
    "\n",
    "ByteLevelBPE_tokenizer_es = ByteLevelBPETokenizer(\n",
    "    vocab_file=f'{path_to_ByteLevelBPE_tokenizer_es_rep}/vocab.json',\n",
    "    merges_file=f'{path_to_ByteLevelBPE_tokenizer_es_rep}/merges.txt'\n",
    ")\n",
    "\n",
    "# Get sequence length max of 1024\n",
    "ByteLevelBPE_tokenizer_es.enable_truncation(max_length=1024)\n",
    "\n",
    "# 3. Import the tokenizer config files in Spanish into the pre-trained GPT2 Tokenizer\n",
    "tokenizer_es = GPT2TokenizerFast.from_pretrained(\n",
    "    str(path_to_ByteLevelBPE_tokenizer_es_rep), \n",
    "    pad_token='<|endoftext|>')\n",
    "# Get sequence length max of 1024\n",
    "tokenizer_es.model_max_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.text.all import *\n",
    "# 1. GPT2TokenizerFast (imported GPT-2 tokenizer) → fastai Tokenizer\n",
    "class TransformersTokenizer(Transform):\n",
    "    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
    "    def encodes(self, x): \n",
    "        toks = self.tokenizer.tokenize(x)\n",
    "        return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n",
    "    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))\n",
    "\n",
    "tokenizer_fastai_en = TransformersTokenizer(tokenizer_en)\n",
    "tokenizer_fastai_es = TransformersTokenizer(tokenizer_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9_NCZwBNub1"
   },
   "outputs": [],
   "source": [
    "# Get weights of the old wte\n",
    "old_wgts = model_en.transformer.get_input_embeddings().weight.clone().detach()\n",
    "\n",
    "# Get the mean embedding vetor of the old wte\n",
    "wgts_m = old_wgts.mean(0)\n",
    "\n",
    "# Initialize vocab size and weights of the new wte\n",
    "new_vocab_size = tokenizer_fastai_es.tokenizer.vocab_size\n",
    "new_wgts = old_wgts.new_zeros(new_vocab_size,old_wgts.size(1))\n",
    "\n",
    "# Get the new wte keeping the embeddings vetors of tokens in common in the 2 vocabs\n",
    "# A token present in the new vocab but not in the old one gets the mean embedding vetor of the old wte\n",
    "old_vocab = tokenizer_fastai_en.tokenizer.get_vocab()\n",
    "new_vocab = tokenizer_fastai_es.tokenizer.get_vocab()\n",
    "\n",
    "if GET_SHARED_TOKENS:\n",
    "    same_tokens_list = list()\n",
    "    different_tokens_list = list()\n",
    "    for w,idx_new in new_vocab.items():    \n",
    "        idx_old = old_vocab.get(w, -1)\n",
    "        if idx_old>=0:\n",
    "            new_wgts[idx_new] = old_wgts[idx_old]\n",
    "            same_tokens_list.append((w,idx_new))\n",
    "        else:\n",
    "            new_wgts[idx_new] = wgts_m\n",
    "            different_tokens_list.append((w,idx_new))\n",
    "\n",
    "    # setup in model the new wte\n",
    "    new_wte = nn.Embedding(new_vocab_size,old_wgts.size(1))\n",
    "    #new_wte.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n",
    "    new_wte.weight.data = new_wgts\n",
    "    model_en.transformer.set_input_embeddings(new_wte)\n",
    "    print(f'Spanish wte matrix setup done!\\n\\nWe kept {len(same_tokens_list)} embeddings vectors from the English one.\\nWe did not kept {len(different_tokens_list)} embeddings vectors from the English one (instead, we used the old wte mean vector).\\n')\n",
    "\n",
    "    # Check identical tokens between the 2 vocabs               \n",
    "    num = 15\n",
    "    print(f'{num} first tokens IN common between the 2 vocabs:\\n{same_tokens_list[:num]}\\n')\n",
    "    print(f'{num} first tokens NOT in common between the 2 vocabs:\\n{different_tokens_list[:num]}')\n",
    "\n",
    "    # save new_wgts\n",
    "    torch.save(new_wgts, path_data/'new_wte_wgts.es')\n",
    "    # save same_tokens_list and different_tokens_list\n",
    "    torch.save(same_tokens_list, path_data/'same_tokens_list.es')\n",
    "    torch.save(different_tokens_list, path_data/'different_tokens_list.es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imqNRnnuTVEq"
   },
   "outputs": [],
   "source": [
    "# load new_wgts\n",
    "new_wgts = torch.load(path_data/'new_wte_wgts.es')\n",
    "# load same_tokens_list and different_tokens_list\n",
    "same_tokens_list = torch.load(path_data/'same_tokens_list.es')\n",
    "different_tokens_list = torch.load(path_data/'different_tokens_list.es')\n",
    "                      \n",
    "# setup in model the new wte\n",
    "new_wte = nn.Embedding(new_vocab_size, old_wgts.size(1))\n",
    "new_wte.weight.data = new_wgts\n",
    "model_en.transformer.set_input_embeddings(new_wte)\n",
    "print(f'Spanish wte matrix setup done!\\n\\nWe kept {len(same_tokens_list)} embeddings vectors from the English one.\\nWe did not kept {len(different_tokens_list)} embeddings vectors from the English one (instead, we used the old wte mean vector).\\n')\n",
    "\n",
    "# Check identical tokens between the 2 vocabs               \n",
    "num = 15\n",
    "print(f'{num} first tokens IN common between the 2 vocabs:\\n{same_tokens_list[:num]}\\n')\n",
    "print(f'{num} first tokens NOT in common between the 2 vocabs:\\n{different_tokens_list[:num]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the embeddings vetors of the common tokens are the ones from the old wte matrix\n",
    "old_vocab = tokenizer_fastai_en.tokenizer.get_vocab()\n",
    "#new_vocab = tokenizer_fastai_pt.tokenizer.get_vocab()\n",
    "count = 0\n",
    "\n",
    "for (tok,idx) in same_tokens_list:\n",
    "    w = tokenizer_fastai_es.tokenizer.convert_ids_to_tokens(idx)\n",
    "    tens_a = new_wgts[idx]\n",
    "    idx_old = old_vocab.get(w, -1)\n",
    "    if idx_old >= 0:\n",
    "        tens_b = old_wgts[idx_old]\n",
    "    else:\n",
    "        tens_b = wgts_m\n",
    "    if ( torch.all(tens_a.eq(tens_b)) == False) or (w != tok):\n",
    "        print('idx,tok:',idx,tok)\n",
    "        print('idx,w:',idx,w)\n",
    "        print('idx_old:',idx_old)\n",
    "        print('identical?',torch.all(tens_a.eq(tens_b)))\n",
    "        count += 1\n",
    "\n",
    "if count == 0:        \n",
    "    print(f'Great! All the embeddings vetors of the {len(same_tokens_list)} common tokens are the ones of the old wte matrix :-)\\n')\n",
    "    \n",
    "# Check that the embeddings vetors of the NOT common tokens are the old wte mean vetor\n",
    "count = 0\n",
    "\n",
    "for (tok,idx) in different_tokens_list:\n",
    "    w = tokenizer_fastai_es.tokenizer.convert_ids_to_tokens(idx)\n",
    "    tens_a = new_wgts[idx]\n",
    "    idx_old = old_vocab.get(w, -1)\n",
    "    if idx_old >= 0:\n",
    "        tens_b = old_wgts[idx_old]\n",
    "    else:\n",
    "        tens_b = wgts_m\n",
    "    if ( torch.all(tens_a.eq(tens_b)) == False) or (w != tok):\n",
    "        print('idx,tok:',idx,tok)\n",
    "        print('idx,w:',idx,w)\n",
    "        print('idx_old:',idx_old)\n",
    "        print('identical?',torch.all(tens_a.eq(tens_b)))\n",
    "        count += 1\n",
    "\n",
    "if count == 0:        \n",
    "    print(f'Great! All the embeddings vetors of the {len(different_tokens_list)} NOT common tokens are the old wte mean vetor :-)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_en.lm_head.weight = model_en.transformer.wte.weight\n",
    "# Check atual weight of wte and lm_head and if wte = lm_head\n",
    "tens_a = model_en.transformer.wte.weight\n",
    "tens_b = model_en.lm_head.weight\n",
    "model_en.transformer.wte.weight,model_en.lm_head.weight,torch.all(tens_a.eq(tens_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create fastai v2 Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'es'\n",
    "fname = f'all_texts_{lang}wiki.csv'\n",
    "df = pd.read_csv(path_data/fname)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE=False\n",
    "if SAMPLE:\n",
    "    df_sample = df[:1000]\n",
    "\n",
    "    num = int(0.8*len(df_sample))\n",
    "\n",
    "    idxs = np.random.randint(0, len(df_sample), len(df_sample))\n",
    "    idxs_train = idxs[:num]\n",
    "    idxs_val = idxs[num:]\n",
    "\n",
    "    all_texts = np.concatenate([df_sample.iloc[idxs_train].text.values, df_sample.iloc[idxs_val].text.values])\n",
    "    splits = [list(idxs_train), list(idxs_val)]\n",
    "    tls = TfmdLists(all_texts, TransformersTokenizer(tokenizer_es), splits=splits, dl_type=LMDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = int(0.8*len(df))\n",
    "\n",
    "idxs = np.random.randint(0, len(df), len(df))\n",
    "idxs_train = idxs[:num]\n",
    "idxs_val = idxs[num:]\n",
    "\n",
    "#save idxs train and valid\n",
    "torch.save(idxs_train, path_data/'idxs_train.es')\n",
    "torch.save(idxs_val, path_data/'idxs_val.es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load idxs train and valid\n",
    "idxs_train = torch.load(path_data/'idxs_train.es')\n",
    "idxs_val = torch.load(path_data/'idxs_val.es')\n",
    "all_texts = np.concatenate([df.iloc[idxs_train].text.values, df.iloc[idxs_val].text.values])\n",
    "splits = [list(idxs_train), list(idxs_val)]\n",
    "tls = TfmdLists(all_texts, TransformersTokenizer(tokenizer_es), splits=splits, dl_type=LMDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TOKENIZE:\n",
    "    def tokenize(text):\n",
    "        toks = tokenizer_es.tokenize(text)\n",
    "        return tensor(tokenizer_es.convert_tokens_to_ids(toks))\n",
    "    tokenized = [tokenize(t) for t in progress_bar(all_texts)]\n",
    "    torch.save(tokenized, path_data/'tokenized_gpt2.es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_es = torch.load(path_data/'tokenized_gpt2.es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the GPT-2 model was trained with sequences of size 1024, we use this sequence length (it's a stateless model, so it will change the perplexity if we use less).\n",
    "bs,sl = 8,1024\n",
    "dls = tls.dataloaders(bs=bs, seq_len=sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersTokenizer(Transform):\n",
    "    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
    "    def encodes(self, x): \n",
    "        return x if isinstance(x, Tensor) else tokenize(x)\n",
    "        \n",
    "    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))\n",
    "\n",
    "class DropOutput(Callback):\n",
    "    def after_pred(self): self.learn.pred = self.pred[0]\n",
    "\n",
    "def splitter(model):\n",
    "    \"Split a GPT2 `model` in 3 groups for differential learning rates.\"\n",
    "    \n",
    "    # First layers group : decoder blocks from 0 to 3\n",
    "    modules = []\n",
    "    for i in range(4): modules.append(model.transformer.h[i])\n",
    "    groups = [nn.Sequential(*modules)]\n",
    "\n",
    "    # Second layers group : decoder blocks from 4 to 7\n",
    "    modules = []\n",
    "    for i in range(4,8,1): modules.append(model.transformer.h[i])\n",
    "    groups = L(groups + [nn.Sequential(*modules)])\n",
    "\n",
    "    # Third layers group : decoder blocks from 8 to 11\n",
    "    modules = []\n",
    "    for i in range(8,12,1): modules.append(model.transformer.h[i])\n",
    "    groups = L(groups + [nn.Sequential(*modules)])\n",
    "    \n",
    "    # Fourth layers group : embeddings matrices wte and wpe + LayerNorm at the model output\n",
    "    groups = L(groups + [nn.Sequential(model.transformer.wte,model.transformer.wpe,model.transformer.ln_f)])\n",
    "    \n",
    "    return groups.map(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner: basic class for handling the training loop\n",
    "# source: https://dev.fast.ai/learner#Learner\n",
    "learn = Learner(dls, model_en, loss_func=CrossEntropyLossFlat(),\n",
    "                splitter = splitter,\n",
    "                cbs=[DropOutput], \n",
    "                metrics=[accuracy, Perplexity()]).to_fp16()\n",
    "learn.create_opt()\n",
    "print(f'number of parameters groups: {len(learn.opt.param_groups)}')\n",
    "\n",
    "# ... and the list of Learning Rates (before its atualization by the Optimizer of the function fit_one_cycle())\n",
    "for i,h in enumerate(learn.opt.hypers):\n",
    "    print(i,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, accuracy, Perplexity() of validation dataset\n",
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze all layers but the last layers group (do not freeze wte, wpe embeddings matrices and last LayerNorm)\n",
    "learn.freeze()\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 2e-3)\n",
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(path_data/'GPT2_es_1epoch_lr2e-3')\n",
    "learn = learn.load(path_data/'GPT2_es_1epoch_lr2e-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-3/(2.6**4),1e-3))\n",
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(path_data/'GPT2_es_2epoch_lr1e-3')\n",
    "learn = learn.load(path_data/'GPT2_es_2epoch_lr1e-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-4/(2.6**4),5e-4))\n",
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(path_data/'GPT2_es_3epoch_lr5e-4')\n",
    "learn = learn.load(path_data/'GPT2_es_3epoch_lr5e-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-4/(2.6**4),1e-4))\n",
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(path_data/'GPT2_es_5epoch_lr1e-4_v2')\n",
    "learn = learn.load(path_data/'GPT2_es_5epoch_lr1e-4_v2')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Train from wiki",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
